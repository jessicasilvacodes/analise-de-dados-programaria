# -*- coding: utf-8 -*-
"""notebook_analisedados-PrograMaria.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14dbZZgQUBd8FlZ2lCRw6C8-ut0xRnKHo

# **Análise de dados - curso PrograMaria**

#Introdução

Esse projeto analisa os dados da pesquisa anual do DataHackers, juntamente com dados fictícios fornecidos pela PrograMaria, que explora tendências, desafios e diversidade no mercado de Tecnologia da Informação (TI) no Brasil. Essa pesquisa abrange diversos aspectos, como perfil dos profissionais, formação acadêmica, áreas de atuação, salários, ferramentas utilizadas, e questões relacionadas à diversidade, como gênero e etnia.

O objetivo principal é aplicar técnicas de análise de dados para extrair insights valiosos, identificando padrões, tendências e possíveis correlações para compreender melhor o cenário atual do mercado de TI no Brasil, seus desafios e oportunidades, e discutir sobre o desenvolvimento e a diversidade no setor.

#Análise de Dados com Python

##Primeira Análise e Biblioteca Pandas

Importando bibliotecas:
"""

from google.colab import drive

import pandas as pd

"""Buscando a planilha de dados:"""

dados = pd.read_excel('/content/drive/MyDrive/curso Analise de Dados - PrograMaria/planilha_m3_analisedados-PrograMaria.xlsx')

"""Visualizando os dados:"""

dados.head()

dados.tail()

dados.shape

"""Visualizando informações da tabela:"""

dados.info()

"""Visualizando os dados estatísticos dos valores numéricos:"""

dados.describe()

"""###Repetindo as análises da planilha do Excel

Nomes das colunas:
"""

dados.columns

"""Filtrando tudo que está na coluna de 'Gênero' e que seja 'Feminino':"""

dados_feminino = dados[dados['GENERO']=='Feminino']

dados_feminino.head()

"""Mulheres com 30 anos ou mais:"""

dados[(dados['IDADE']>=30) & (dados['GENERO']=='Feminino')]

"""Quantidade de pessoas por gênero (soma de valores):"""

dados.groupby('GENERO')['ID'].nunique()

"""Visualizando a quantidade de valores nulos:"""

dados.groupby('GENERO', dropna=False)['ID'].nunique()

"""Uma outra forma de visualizar..."""

dados['GENERO'].value_counts(dropna=False)

"""Visualizando as pessoas acima de 30 anos e por nível de cargo:"""

dados[dados['IDADE']>=30]['NIVEL'].value_counts()

"""Agora somente mulheres, com 30 anos ou mais e divididas por nível de cargo:"""

dados[(dados['IDADE']>=30) & (dados['GENERO']=='Feminino')]['NIVEL'].value_counts()

"""Analisando a quantidade de pessoas de cada gênero por nível de cargo:"""

pd.pivot_table(dados, values=['ID'], index=['GENERO'], columns=['NIVEL'], aggfunc='count')

"""Analisando a quantidade de gestores de cada gênero:"""

pd.pivot_table(dados, values=['ID'], index=['GENERO'], columns=['GESTOR?'], aggfunc='count')

"""##Análises Estatísticas e Biblioteca Numpy

###Introdução:

Importando bibliotecas:
"""

import numpy as np

lista_idades = [26, 30, 32, 22, 26, 35, 40, 20, 43, 31, 23]
lista_idades

soma_idades = np.sum(lista_idades)
print(f"Soma: {soma_idades}")

media_idades = np.mean(lista_idades)
print(f"Média aritmética: {media_idades}")

mediana_idades = np.median(lista_idades)
print(f"Mediana: {mediana_idades}")

"""Colocar a lista de forma ordenada:"""

lista_idades.sort()
lista_idades

"""###Análises Estatísticas da planilha do Excel:

Avaliando os valores de idade:

Média:
"""

dados['IDADE'].mean()

"""Mediana:"""

dados['IDADE'].median()

"""Idade que mais aparece:"""

dados['IDADE'].mode()

"""Desvio Padrão:

*Quanto mais próximo de zero, mais uniforme e mais próximos da média os dados estão.*
"""

dados['IDADE'].std()

"""Idade máxima:"""

dados['IDADE'].max()

"""Idade mínima:"""

dados['IDADE'].min()

"""Variância:"""

dados['IDADE'].var()

"""Média de idade do gênero feminino:"""

dados[dados['GENERO']=='Feminino']['IDADE'].mean()

"""Média de idade do gênero masculino:"""

dados[dados['GENERO']=='Masculino']['IDADE'].mean()

"""Avaliando os valores de salário:

Média:
"""

dados['SALARIO'].mean()

"""Mediana:"""

dados['SALARIO'].median()

"""Valor máximo:"""

dados['SALARIO'].max()

"""Valor mínimo:"""

dados['SALARIO'].min()

"""Desvio Padrão:"""

dados['SALARIO'].std()

"""Média de salário de pessoas do gênero feminino:"""

dados[dados['GENERO']=='Feminino']['SALARIO']

dados[dados['GENERO']=='Feminino']['SALARIO'].mean()

"""Média de salário de pessoas do gênero masculino:"""

dados[dados['GENERO']=='Masculino']['SALARIO']

dados[dados['GENERO']=='Masculino']['SALARIO'].mean()

"""##Tratando valores faltantes (nulos):"""

dados.info()

"""###Tratando a coluna 'GENERO':

Mostrar os valores nulos:
"""

dados.groupby('GENERO', dropna=False)['ID'].nunique()

"""Preencher os nulos:"""

dados['GENERO'] = dados['GENERO'].fillna("Prefiro não informar")

"""Verificando:"""

dados.groupby('GENERO', dropna=False)['ID'].nunique()

"""###Tratando a coluna 'IDADE':

Mostrar os valores nulos:
"""

dados['IDADE'].isnull().value_counts()

"""Observando a coluna 'FAIXA IDADE' e analisando se todas as pessoas que deixaram 'IDADE' nulo, também deixaram 'FAIXA IDADE' nulo:"""

dados[dados['IDADE'].isnull()]['FAIXA IDADE'].value_counts()

"""Vamos pegar as médias das idades entre 17-21 e substituir os 6 valores ausentes por essa média, e depois pegar as médias das idades 55+ e substituir os 68 valores pela respectiva média.

**Primeiro, faixa de idade '17-21':**

Vamos filtrar os valores e tirar a média:
"""

media_17_21 = dados[dados['FAIXA IDADE']=='17-21']['IDADE'].mean()
media_17_21

"""Localizando onde estão os dados nulos da tabela 'IDADE':"""

dados.loc[(dados['FAIXA IDADE']=='17-21') & (dados['IDADE'].isnull()), 'IDADE']

"""Substituindo...:"""

dados.loc[(dados['FAIXA IDADE']=='17-21') & (dados['IDADE'].isnull()), 'IDADE'] = media_17_21

"""Verificando...:"""

dados[dados['IDADE'].isnull()]['FAIXA IDADE'].value_counts()

"""**Agora, faixa de idade '55+':**"""

dados[dados['FAIXA IDADE']=='55+']['IDADE']

"""Nenhuma das pessoas com a faixa de idade 55+, colocou o valor real da idade. Então vamos usar a média geral para preecher esses valores."""

media_geral = dados['IDADE'].mean()
media_geral

"""Localizar os dados nulos:"""

dados.loc[(dados['FAIXA IDADE']=='55+') & (dados['IDADE'].isnull()), 'IDADE']

"""Substituindo...:"""

dados.loc[(dados['FAIXA IDADE']=='55+') & (dados['IDADE'].isnull()), 'IDADE'] = media_geral

"""Verificando...:"""

dados[dados['IDADE'].isnull()]['FAIXA IDADE'].value_counts()

"""###Tratando a coluna 'SALARIO':"""

dados[dados['SALARIO'].isnull()]

"""557 linhas nulas na coluna 'SALARIO'.

Analisando a coluna 'FAIXA SALARIAL':
"""

dados[dados['SALARIO'].isnull()]['FAIXA SALARIAL'].value_counts()

"""Podemos observar que no momento de responder o formulário, as pessoas deixaram a coluna 'SALARIO' e a coluna 'FAIXA SALARIAL' sem informações.

Então vamos utilizar a mediana dos salários para preencher os valores nulos.

Nesses casos, é melhor utilizar a mediana do que a media, pois pode ser que haja alguns valores discrepantes na coluna 'SALARIO'.
"""

mediana_salario = dados['SALARIO'].median()
mediana_salario

"""Localizando onde estão os valores nulos:"""

dados.loc[dados['SALARIO'].isnull(), 'SALARIO']

"""Substituindo...:"""

dados.loc[dados['SALARIO'].isnull(), 'SALARIO'] = mediana_salario

"""Verificando...:"""

dados['SALARIO'].isnull().value_counts()

"""Nenhum valor True, ou seja, nenhum valor nulo."""



"""##Tratando valores discrepantes (outliers):

Importando bibliotecas:
"""

import matplotlib.pyplot as plt

plt.boxplot(dados['SALARIO'])

"""Observamos alguns outliers...

Calculando os limites = partil = quartis:
"""

Q1 = dados['SALARIO'].quantile(0.25)
Q1

Q3 = dados['SALARIO'].quantile(0.75)
Q3

"""Interquartil:"""

IQR = Q3-Q1
IQR

"""Limites superiores = Q3 + (1.5*IQR)"""

lim_superior = Q3 + (1.5*IQR)
lim_superior

"""Limites inferiores = Q1 - 1.5*IQR"""

lim_inferior = Q1 - (1.5*IQR)
lim_inferior

dados['FAIXA SALARIAL'].value_counts()

media_salario = dados['SALARIO'].mean()
media_salario

desvio_salario = dados['SALARIO'].std()
desvio_salario

limite_superior = media_salario + (3*desvio_salario)
limite_superior

"""Para resolver esses outliers, podemos substituir os valores por estatísticas robustas, como a mediana.

Vamos analisar se as pessoas com salario acima do limite superior marcaram uma faixa salarial correspondente.
"""

dados[dados['SALARIO']>limite_superior]['FAIXA SALARIAL'].value_counts()

"""**Filtro da faixa salarial de 30 a 40mil:**"""

media_30_40 = dados[(dados['FAIXA SALARIAL']=='de R$ 30.001/mês a R$ 40.000/mês') & (dados['SALARIO']<limite_superior)]['SALARIO'].mean()
media_30_40

"""Substituindo...:"""

dados.loc[(dados['FAIXA SALARIAL']=='de R$ 30.001/mês a R$ 40.000/mês') & (dados['SALARIO']>limite_superior), 'SALARIO']

dados.loc[(dados['FAIXA SALARIAL']=='de R$ 30.001/mês a R$ 40.000/mês') & (dados['SALARIO']>limite_superior), 'SALARIO'] = media_30_40

"""Verificando...:"""

dados[dados['SALARIO']>limite_superior]['FAIXA SALARIAL'].value_counts()

"""**Filtro da faixa salarial acima de 40mil:**"""

media_40 = dados[(dados['FAIXA SALARIAL']=='Acima de R$ 40.001/mês') & (dados['SALARIO']<limite_superior)]['SALARIO'].mean()
media_40

"""Substituindo...:"""

dados.loc[(dados['FAIXA SALARIAL']=='Acima de R$ 40.001/mês') & (dados['SALARIO']>limite_superior), 'SALARIO']

dados.loc[(dados['FAIXA SALARIAL']=='Acima de R$ 40.001/mês') & (dados['SALARIO']>limite_superior), 'SALARIO'] = media_40

"""Verificando...:"""

dados[dados['SALARIO']>limite_superior]['FAIXA SALARIAL'].value_counts()

"""Fazendo um novo boxplot:"""

plt.boxplot(dados['SALARIO'])

"""##Intervalo de confiança e distribuição amostral:"""

salarios = dados['SALARIO']
salarios

media_amostral = np.mean(salarios)
media_amostral

desvio_amostral = np.std(salarios)
desvio_amostral

"""Para uma média salarial de 9904, o valor de desvio padrão está muito alto.

Vamos trabalhar com um nível de confiança de 95%:
"""

nivel_confianca = 0.95

tamanho_amostral = len(salarios)
tamanho_amostral

"""Vamos importar um módulo de funções estatísticas:

Importando bibliotecas:
"""

from scipy import stats

erro_padrao = stats.sem(salarios)
erro_padrao

intervalo_confianca = stats.t.interval(nivel_confianca, tamanho_amostral-1, loc=media_amostral, scale=erro_padrao)
intervalo_confianca

"""Ou seja, de acordo com os nossos dados, temos *95% de confianç*a de que a média salarial dos dados é de 9655 reais a 10153 reais.

##Featuring Engineering: criar novas variáveis.

Vamos criar uma função que vai receber os dados de gestor e de nível, e vai retornar uma nova coluna informando se a pessoa é gestora ou qual o nível de cargo que ela tem.
"""

def preencher_nivel(gestor, nivel):
  if gestor == 1:
    return 'Pessoa gestora'
  else:
    return nivel

dados.columns

dados['NOVO NIVEL'] = dados.apply(lambda x: preencher_nivel(x['GESTOR?'], x['NIVEL']), axis=1)     #axis=1 É UM PARÂMETRO PARA APLICAR A FUNÇÃO APPLY NA LINHA E NÃO NA COLUNA

dados['NOVO NIVEL']

dados['NOVO NIVEL'].value_counts()

"""Comparando com a tabela antiga...:"""

dados['NIVEL']

"""Vamos transformar essas variáveis categóricas em um indicador (sim ou não / True or False)."""

dados = pd.get_dummies(dados, columns=['NIVEL'])
dados

"""Pronto, foi adicionado à tabela mais três colunas dividindo o nível em: nível junior, nível pleno e nível senior.

Agora vamos criar uma função que receba a idade e retorne qual de geração a pessoa é.
"""

def determinar_geracao(idade):
  if 13 < idade <= 29:
    return 'Geração Z'
  elif 29 < idade <= 39:
    return 'Millenial'
  elif 39 < idade <= 58:
    return 'Geração X'
  else:
    return 'Outra Geração'

dados['GERACAO'] = dados['IDADE'].apply(determinar_geracao)

dados['GERACAO'].value_counts()

"""##Featuring Engineering: incluir uma nova tabela à nossa base de dados (merge)."""

from google.colab import drive
drive.mount('/content/drive')

dados2 = pd.read_excel('/content/drive/MyDrive/curso Analise de Dados - PrograMaria/planilha_m4_analisedados-PrograMaria.xlsx')

dados2.columns

"""Dado em comum com as duas tabelas: **coluna ID**.

Vamos juntar as duas tabelas:
"""

dados = dados.merge(dados2, on='ID', how='left')

"""Observamos a tabela 'dados' agora completa:"""

dados.head()

dados.columns

"""Vamos analisar alguns dados..."""

dados['Você pretende mudar de emprego nos próximos 6 meses?'].value_counts()

"""Com as informações dessa coluna, podemos criar mais duas colunas, uma informando se a pessoa está buscando emprego e outra informando se a pessoa está aberto a oportunidades ou não.

Por exemplo, sempre que tiver um *'em busca'* na frase, podemos transformar em True. Ou seja, a pessoa está buscando emprego.
"""

dados['EM BUSCA'] = dados['Você pretende mudar de emprego nos próximos 6 meses?'].str.contains('em busca', case=False)

dados['EM BUSCA'].value_counts()

"""E agora, uma coluna de quem está aberto a oportunidades:"""

dados['ABERTO OPORTUNIDADES'] = dados['Você pretende mudar de emprego nos próximos 6 meses?'].str.contains('aberto', case=False)

dados['ABERTO OPORTUNIDADES'].value_counts()

"""Agora vamos trabalhar um pouco com as **etnias**:"""

dados['COR/RACA/ETNIA'].value_counts()

"""Criar uma nova coluna de Etnia: branca e excluir as etnias não brancas/outras.

**Etnia: branca**
"""

dados['Etnia: branca'] = dados['COR/RACA/ETNIA']=='Branca'

dados['Etnia: branca'].value_counts()

"""**Etnia: não branca/outras**"""

dados['Etnia: não branca/outras'] = dados['COR/RACA/ETNIA']!='Branca'

dados['Etnia: não branca/outras'].value_counts()

"""Verificando as novas colunas..."""

dados.columns

"""##Correlação, diferentes funções para dados discretos e contínuos:

**Correlação de Pearson**:

Correlação positiva: quando duas variáveis aumentam juntas.

Correlação negativa: uma variável aumenta enquanto a outra diminui.

Valor próximo de 1 = forte correlação positiva.

Valor próximo de -1 = forte correlação negativa.

Valor próximo de 0 = não há uma correlação linear entre as variáveis.

**Pergunta 1: Quanto mais uma pessoa envelhece, maior o salário dela?**
"""

correlacao_continua = dados['IDADE'].corr(dados['SALARIO'])
correlacao_continua

"""Valor maior que 0! Correlação positiva, porém não é forte.

**Correlação entre educação e etnia:**

Em variáveis categoricas, utilizamos o coeficiente de Cramer (0 ou 1, sendo que 0 indica nenhuma associação, e 1 indica associação completa).
"""

tabela_cruzada = pd.crosstab(dados['COR/RACA/ETNIA'], dados['NIVEL DE ENSINO'])
tabela_cruzada

"""Precisamos dessa tabela em forma de matriz:"""

np.array(tabela_cruzada)

"""A função "chi2_contingency" compara a distribuição observada na tabela cruzada com uma distribuição esperada se as duas variáveis fossem independentes uma da outra.

Quanto maior a diferença entre a distribuição observada e esperada, maior será o valor do *qui quadrado*, o que indica uma associação mais forte entre as variáveis.
"""

from scipy.stats import chi2_contingency

def cramer_coeficiente(coluna1, coluna2):
  tabela_cruzada = np.array(pd.crosstab(coluna1, coluna2))
  chi2 = chi2_contingency(tabela_cruzada)[0]     # 0 pois queremos somente o primeiro resultado
  soma = np.sum(tabela_cruzada)
  mini = min(tabela_cruzada.shape)-1              # padrão da fórmula
  cramer = np.sqrt(chi2/(soma*mini))
  return cramer

cramer_coeficiente(dados['COR/RACA/ETNIA'], dados['NIVEL DE ENSINO'])

"""Com esse resultado, podemos dizer que as colunas de etnia e nível de ensino quase não tem correlação pois pessoas não brancas não tem as mesmas oportunidades de ensino que as pessoas brancas tem. Porém, observams na nossa sociedade que existe sim essa correlação. Então porque na nossa análise a nossa correlação foi tão baixa?

Podemos supor que as pessoas que estão na área de tecnologia já possuem algum nível de formação.

#Salvando em .csv:
"""

dados.to_csv('/content/drive/MyDrive/curso Analise de Dados - PrograMaria/analise_dadostratados_PrograMaria.csv', index=False)

"""#Conectando SQL com Pandas:"""

import sqlite3

sql_conexao = sqlite3.connect('/content/drive/MyDrive/curso Analise de Dados - PrograMaria/status_brasil-PrograMaria')

"""Fazendo a mesma consulta que fizemos no DBEAVER (SQL):

Criar uma query:
"""

query1 = "SELECT * FROM Municipios_Brasileiros WHERE Cidade LIKE 'Itaqua%';"

query1

"""Vamos colocar 2 parâmetros, um deles é a nossa query e a outra é um parâmetro '*con*' que é a nossa variável de conexão com SQL:"""

pd.read_sql(query1, con=sql_conexao)

"""###Trabalhando com SQL:

Primeiro vamos analisar como estão nossas tabelas no banco de dados, depois agrupar por estado com a média do índice de renda e listar (usando *list* e *.unique*).
"""

query2 = '''SELECT Municipios_Brasileiros.Estado, AVG(Municipio_Status.renda)
FROM Municipios_Brasileiros
INNER JOIN Municipio_Status ON Municipios_Brasileiros.municipio_ID = Municipio_Status.municipio_ID
GROUP BY Municipios_Brasileiros.Estado;'''

query2

pd.read_sql(query2, con=sql_conexao)

"""Agora temos a média do índice de renda por estado brasileiro.

Nos nossos dados da pesquisa do DataHackers, nós não temos todos os estados do Brasil. Então iremos listar todos os estados que estão presentes no nosso branco de dados da pesquisa e fazer a consulta por cima dessa lista.

Iremos usar o arquivo .csv que salvamos anteriormente.
"""

dados = pd.read_csv('/content/drive/MyDrive/curso Analise de Dados - PrograMaria/analise_dadostratados_PrograMaria.csv')

dados.columns

"""Vamos listar somente os estados que estão presentes (não repetidos):"""

dados['UF ONDE MORA'].unique()

"""Vamos transformar em uma lista:"""

lista_estados = list(dados['UF ONDE MORA'].unique())
lista_estados

"""Vamos refazer a nossa query com uma condicional = só quero os resultados dos estados que estão dentro da lista que criamos."""

query3 = '''SELECT Municipios_Brasileiros.Estado, AVG(Municipio_Status.renda)
FROM Municipios_Brasileiros
INNER JOIN Municipio_Status ON Municipios_Brasileiros.municipio_ID = Municipio_Status.municipio_ID
WHERE Municipios_Brasileiros.Estado IN ({})
GROUP BY Municipios_Brasileiros.Estado;'''.format(','.join(['?' for _ in lista_estados]))

query3

estados_renda = pd.read_sql(query3, con=sql_conexao, params=lista_estados)
estados_renda

"""Agora sim, temos uma lista somente com os estados presente na pesquisa do DataHackers e com a média do índice de renda de cada estado.

Vamos renomear a coluna 'UF ONDE MORA' para facilitar a análise:
"""

dados.rename(columns={'UF ONDE MORA':'Estado'}, inplace=True)

dados.columns

"""Agora vamos mergear as duas colunas no nosso banco de dados:"""

dados = dados.merge(estados_renda, on='Estado', how='left')

dados.head()

"""###Agora vamos fazer mais uma análise de correlação:"""

correlacao_renda_salario = dados['SALARIO'].corr(dados['AVG(Municipio_Status.renda)'])
correlacao_renda_salario

"""No contexto dessa pesquisa e dessa amostra de pessoas que responderam o formulário, a correlação entre renda e salário é quase 0."""



"""#Visualização de Dados com Python:"""

dados

"""###Quantidade de pessoas por gênero:"""

genero_counts = dados['GENERO'].value_counts()
genero_counts

import matplotlib.pyplot as plt

"""Gráfico feito com a biblioteca *matplotlib*:"""

plt.figure(figsize=(8, 4))
plt.bar(height = genero_counts.values, x = genero_counts.index)
plt.title('QUANTIDADE DE PESSOAS POR GÊNERO')
plt.xlabel('GÊNERO')
plt.ylabel('CONTAGEM')
plt.show()

"""O mesmo gráfico, agora feito com a biblioteca *seaborn*:"""

import seaborn as sns

plt.figure(figsize=(8, 4))
sns.countplot(data=dados, x='GENERO', palette='mako')
plt.title('QUANTIDADE DE PESSOAS POR GÊNERO')
plt.xlabel('GÊNERO')
plt.ylabel('CONTAGEM')
plt.grid(False)
plt.show

"""###Média de salário por idade:"""

salario_por_idade = dados.groupby('IDADE')['SALARIO'].mean()
salario_por_idade

"""Gráfico feito com a biblioteca *matplotlib*:"""

plt.figure(figsize=(8, 4))
plt.plot(salario_por_idade.index, salario_por_idade.values, marker='o', linestyle='-')
plt.title('MÉDIA DE SALÁRIO POR IDADE')
plt.xlabel('IDADE')
plt.ylabel('MÉDIA DE SALÁRIO')
plt.grid(True)
plt.show()

"""O mesmo gráfico, agora feito com a biblioteca *plotly*:"""

import plotly.express as px

fig = px.line(salario_por_idade.reset_index(), x='IDADE', y='SALARIO', title='MÉDIA DE SALÁRIO POR IDADE', markers=True, color_discrete_sequence=px.colors.qualitative.Dark24)
fig.show()

"""Vamos fazer esse gráfico do tipo scatter:"""

plt.figure(figsize=(8, 4))
plt.scatter(x=dados['IDADE'], y=dados['SALARIO'], alpha=0.5)
plt.xlabel('IDADE')
plt.ylabel('SALÁRIO')
plt.title('MÉDIA DE SALÁRIO POR IDADE')
plt.grid(True)
plt.show()

"""Agora vamos fazer o mesmo gráfico scatter usando a biblioteca *plotly*:"""

fig2 = px.scatter(dados, x='IDADE', y='SALARIO', title='MÉDIA DE SALÁRIO POR IDADE', color_discrete_sequence=px.colors.qualitative.Dark24)
fig2.show()

"""Resumindo: o gráfico de linha ficou muito melhor para a visualização e entendimento dos dados.

# Aprendizado de Máquina / Machine Learning

O modelo de regressão que faremos será apenas com os dados das pessoas que estão empregadas do tipo CLT.
1. Filtrar da coluna de “QUAL SUA SITUAÇÃO ATUAL DE TRABALHO?” todas as pessoas que marcaram a opção “Empregado (CLT)”.
2. Criar uma coluna chamada “NÃO_BRANCA”, onde, se na coluna de “COR/RACA/ETNIA” for “branca” essa coluna recebe 0, e caso contrário recebe 1.
3. Na coluna “QUANTO TEMPO DE EXPERIÊNCIA NA ÁREA DE DADOS VOCÊ TEM?”, vamos usar a função *extract* para pegar o primeiro dígito que aparece em cada opção dessa coluna e aplicar o mesmo para a coluna de “NÚMERO DE FUNCIONÁRIOS”.
"""

dados = pd.read_excel('/content/drive/MyDrive/curso Analise de Dados - PrograMaria/planilha_m7_analisedados-PrograMaria.xlsx')

dados.head()

dados.columns

dados['QUAL SUA SITUAÇÃO ATUAL DE TRABALHO?'].value_counts()

dados = dados[dados['QUAL SUA SITUAÇÃO ATUAL DE TRABALHO?']=='Empregado (CLT)']

dados['QUAL SUA SITUAÇÃO ATUAL DE TRABALHO?'].value_counts()

dados['COR/RACA/ETNIA'].value_counts()

"""Vamos tirar as categorias que tem poucos dados (indígena, outra e prefiro não informar), pois ao olhar nosso relatório de dashboard, vemos que essas categorias ficaram com uma média salarial super alta, porém são pouquíssimos dados amostrais e não representam a população de forma geral (outliers)."""

lista_retirar = ['Prefiro não informar', 'Outra', 'Indígena']

dados = dados[~dados['COR/RACA/ETNIA'].isin(lista_retirar)]

dados['NAO_BRANCA'] = dados['COR/RACA/ETNIA'].apply(lambda x: 1 if x != 'Branca' else 0)

dados['QUANTO TEMPO DE EXPERIÊNCIA NA ÁREA DE DADOS VOCÊ TEM?'].value_counts()

dados['TEMPO_EXPERIENCIA'] = dados['QUANTO TEMPO DE EXPERIÊNCIA NA ÁREA DE DADOS VOCÊ TEM?'].str.extract(r'(\d+)')

"""*Explicação: dentro da função extract temos o r que indica uma string crua, onde caracteres especiais são tratados literalmente. Isso é útil para evitar confusão com caracteres em expressões regulares. O \d é um metacaracter e representa qualquer dígito entre 0 a 9. O + é um quantificador que indica 1 ou + (somente dígitos). Resumindo, isso indica 1 ou mais dígitos.*"""

dados['TEMPO_EXPERIENCIA'].value_counts()

dados['TEMPO_EXPERIENCIA'].value_counts(dropna=False)

dados['TEMPO_EXPERIENCIA'] = dados['TEMPO_EXPERIENCIA'].fillna(0)

dados['TEMPO_EXPERIENCIA'].value_counts()

dados['NUMERO DE FUNCIONARIOS'].value_counts()

dados['NUMERO DE FUNCIONARIOS'] = dados['NUMERO DE FUNCIONARIOS'].str.replace('.', '')

dados['NUMERO DE FUNCIONARIOS'].value_counts()

dados['NUMERO DE FUNCIONARIOS'] = dados['NUMERO DE FUNCIONARIOS'].str.extract(r'(\d+)')

dados['NUMERO DE FUNCIONARIOS'].value_counts(dropna=False)

"""1. Agora, vamos criar uma coluna chamada “INSATISFACAO”, a partir da coluna existente 'Qual o principal motivo da sua insatisfação com a empresa atual?', com o valor igual a 1 para toda frase que continha a palavra “Salário”.

2. vamos substituir cada categoria da coluna de ‘NIVEL DE ENSINO’ por um valor. Na ordem crescente de escolaridade, por exemplo: 0 para “Não tenho graduação formal”, 1 para “Estudante de Graduação”, 2 para “Graduação-Bacharelado”, e assim por diante...
"""

dados['Qual o principal motivo da sua insatisfação com a empresa atual?'].value_counts()

dados['INSATISFACAO'] = 0

dados.loc[dados['Qual o principal motivo da sua insatisfação com a empresa atual?'].notnull(), 'Qual o principal motivo da sua insatisfação com a empresa atual?'].apply(lambda x: 1 if 'Salário' in x else 0)

dados.loc[dados['Qual o principal motivo da sua insatisfação com a empresa atual?'].notnull(), 'INSATISFACAO'] = dados.loc[dados['Qual o principal motivo da sua insatisfação com a empresa atual?'].notnull(), 'Qual o principal motivo da sua insatisfação com a empresa atual?'].apply(lambda x: 1 if 'Salário' in x else 0)

dados['INSATISFACAO'].value_counts()

"""279 pessoas insatisfeitas com o salário de alguma forma."""

dados['NIVEL DE ENSINO'].value_counts()

dados['NIVEL DE ENSINO'] = dados['NIVEL DE ENSINO'].apply(lambda x: 0 if x=='Não tenho graduação formal'
                               else 1 if x=='Estudante de Graduação'
                               else 2 if x=='Graduação/Bacharelado'
                               else 3 if x=='Pós-graduação'
                               else 4 if x=='Mestrado'
                               else 5 if x=='Doutorado ou Phd'
                               else -1)

dados['NIVEL DE ENSINO'].value_counts()

"""1. Selecionar as colunas que serão os argumentos do nosso modelo de regressão: 'IDADE', 'GENERO', 'NAO_BRANCA','TEMPO_EXPERIENCIA', 'SETOR',  'REGIAO ONDE MORA','NIVEL DE ENSINO',  'NUMERO DE FUNCIONARIOS', 'NOVO_NIVEL','SALARIO' e 'INSATISFACAO'.

2. Aplicar o get_dummies para as colunas de 'GENERO', 'REGIAO ONDE MORA' e 'NOVO_NIVEL' (*get_dummies pois as categorias não tem uma ordem crescente como em nivel de ensino*).

3. Dividir os dados em dois conjuntos, um de teste (para avaliação do modelo) e um de treinamento (para o treinamento do modelo).

4. Separar os atributos do nosso objetivo (target = salário), e utilizar uma função chamada *train_test_split* para fazer a divisão dos dados.

5. Usar a função *StandardScaler* para padronizar as características, ou seja, normalizar os nossos dados.

6. Treinar o modelo:
"""

dados = dados[[ 'IDADE', 'GENERO', 'NAO_BRANCA','TEMPO_EXPERIENCIA', 'SETOR',  'REGIAO ONDE MORA','NIVEL DE ENSINO',  'NUMERO DE FUNCIONARIOS', 'NOVO_NIVEL','SALARIO', 'INSATISFACAO']]

dados.columns

dados = pd.get_dummies(dados, columns=['GENERO', 'SETOR', 'REGIAO ONDE MORA', 'NOVO_NIVEL'], drop_first=True)

dados.head()

"""###Treinamento do modelo:"""

X = dados.drop('SALARIO', axis=1)     #atributos
y = dados['SALARIO']      #target

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""- test_size = tamanho do teste, em pocentagem
- random_state = semente para o gerador de numeros aleatórios
"""

from sklearn.preprocessing import StandardScaler

"""Esse módulo tem como objetivo padronizar as características, remover a média e escalar para a variância unitária, ou seja, normalizar os dados.

Vamos criar o objeto de normalização para a parte de treinos (não precisamos apontar quais colunas queremos normalizar):
"""

scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.fit_transform(X_test)

"""Vamos importar a função *LinearRegression* da biblioteca sklearn, criar um objeto chamado *model* apartir dessa função.
Depois, vamos enviar os parâmetros de treino na função fit para treinar o modelo.

- fazer as predições usando a função predict e enviando como parâmetro o conjunto de teste (X_test_scaled);

- avaliar o modelo, para saber se está bom ou ruim focando em três métricas principais:

1. MSE (Mean Squared Error)
2. MAE (Mean Absolute Error)
3. R² (R-squared).

"""

from sklearn.linear_model import LinearRegression

model = LinearRegression()

"""Treinar o modelo:"""

model.fit(X_train_scaled, y_train)

"""Predições:"""

y_pred = model.predict(X_test_scaled)
y_pred

"""Esses valores são as previsões dos valores de salário para cada conjunto de atributos."""

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

"""- MSE (Mean Squared Error) = erro quadrático médio = calcula a média de diferença entre o valor predito e o valor real, e elevado ao quadrado, e dessa maneira penaliza valores que são muito diferentes entre o valor previsto e o valor real."""

mse = mean_squared_error(y_test, y_pred)
mse

"""Podemos observar um valor alto, indicando que o modelo não está com predições precisas.

Então vamos calcular outra métrica, a MAE (Mean Absolute Error) = erro médio absoluto, que calcula a média da diferença absoluta, sem elevar ao quadrado.
"""

mae = mean_absolute_error(y_test, y_pred)
mae

"""Esse valor, para a faixa salarial que temos, é aceitável. Vamos calcular a R-squared.

-  R² (R-squared) = medida estatística de quão próximos os dados estão da linha de regressão = pocentagem da variação da variável resposta que é explicado por um modelo linear = variação explicada dividida pela variação total. Valores entre 0 e 100%.
"""

r2 = r2_score(y_test, y_pred)
r2

"""Devido ao valor absurdamente alto de MSE e com os resultados obtidos, podemos concluir que para um modelo real de produção, esse modelo não seria aceitável.

Vamos plotar um gráfico de dispersão dos valores reais versus os valores preditos pelo modelo:
"""

plt.figure(figsize=(12, 6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.xlabel('VALOR REAIS')
plt.ylabel('VALOR PREDITO')
plt.title('DISPERSÃO DOS VALORES REAIS x VALORES PREDITOS')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red')
plt.show()

"""A linha vermelha representa o nosso modelo de regressão linear, ou seja, os valores preditos. As bolinhas azuis são os valores reais dos salários. Podemos ver que existe uma concentração nos valores abaixo de R$10.000,00. Muitos salários estão dessa faixa.

Podemos analisar que até aproximadamente R$20.000,00, temos valores muito próximos do real com o nosso modelo de predição.
"""

nomes_atributos = X_train.columns
nomes_atributos

coeficientes = pd.DataFrame(model.coef_, columns=['Coeficientes'], index=nomes_atributos)

coeficientes

"""Vamos ver o que está influenciando mais e o que está influenciando menos..."""

coeficientes = coeficientes.sort_values(by='Coeficientes', ascending=False)

coeficientes

coeficientes.plot.barh(figsize=(12, 10))
plt.axvline(x=0, color='red')

"""Podemos ver que o que mais influencia pra esse aumento de salário é se a pessoa é gestora. Depois, se é nível senior, pelo tempo de experiência, nível de ensino, gênero masculino...

Os atributos que tiveram peso pra colocar o salário mais pra baixo, foram: região sul, região nordeste, pessoas não brancas... Confirmando assim, o que todas as nossas analises vem mostrando, por exemplo, que as pessoas não brancas tem um peso na questão do salário.

Observações: temos que ter cuidado com a ética, por exemplo, para o modelo não reproduzir os viéses do mundo real.

###Dashboard para visualização de dados:

[Dashboard feito com o Google Looker](https://lookerstudio.google.com/reporting/cd62189e-065f-403a-8ffa-b82df820ef39)

Notebook criado por [Jessica Silva](https://github.com/jessicasilvacodes).

Análise de Dados - curso [PrograMaria](https://www.programaria.org/).

#Conclusão:

A análise desses dados nos permitiu obter insights significativos sobre o mercado de TI no Brasil. Identificamos tendências importantes relacionadas a salários, áreas de atuação, nível de ensino, e os desafios enfrentados pelos profissionais do setor. Além disso, pudemos observar o impacto das questões de diversidade, incluindo gênero e etnia, e como essas variáveis influenciam a realidade do mercado.

Por meio das visualizações e análises realizadas, fica evidente a importância de iniciativas como a pesquisa DataHackers para fomentar o entendimento do mercado e para promover debates relevantes sobre equidade e inclusão no setor de TI, contribuindo para criar um ambiente de trabalho mais justo e inovador.
"""